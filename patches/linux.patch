diff --git a/Makefile b/Makefile
index 68a8faff2..4a748c136 100644
--- a/Makefile
+++ b/Makefile
@@ -2,7 +2,7 @@
 VERSION = 6
 PATCHLEVEL = 12
 SUBLEVEL = 0
-EXTRAVERSION =
+EXTRAVERSION = ioam-direct-export
 NAME = Baby Opossum Posse
 
 # *DOCUMENTATION*
diff --git a/include/net/ioam6.h b/include/net/ioam6.h
index 2cbbee6e8..fe7f68942 100644
--- a/include/net/ioam6.h
+++ b/include/net/ioam6.h
@@ -39,10 +39,19 @@ struct ioam6_schema {
 	u8 data[];
 };
 
+struct ioam6_dex_flow {
+	struct rhash_head head;
+	struct rcu_head rcu;
+
+	__be32 flowID;
+	__be32 seqID;
+};
+
 struct ioam6_pernet_data {
 	struct mutex lock;
 	struct rhashtable namespaces;
 	struct rhashtable schemas;
+	struct rhashtable dex_flows;
 };
 
 static inline struct ioam6_pernet_data *ioam6_pernet(struct net *net)
@@ -55,6 +64,10 @@ static inline struct ioam6_pernet_data *ioam6_pernet(struct net *net)
 }
 
 struct ioam6_namespace *ioam6_namespace(struct net *net, __be16 id);
+
+/* Insert flow with given id */
+struct ioam6_dex_flow *ioam6_dex_flow_insert(struct net *net, __be32 flowID);
+
 void ioam6_fill_trace_data(struct sk_buff *skb,
 			   struct ioam6_namespace *ns,
 			   struct ioam6_trace_hdr *trace,
@@ -66,7 +79,8 @@ void ioam6_exit(void);
 int ioam6_iptunnel_init(void);
 void ioam6_iptunnel_exit(void);
 
-void ioam6_event(enum ioam6_event_type type, struct net *net, gfp_t gfp,
-		 void *opt, unsigned int opt_len);
+void ioam6_event(enum ioam6_event_type type, struct sk_buff *skb,
+		 struct net *net, struct ioam6_namespace *ns,
+		 gfp_t gfp, void *opt, unsigned int opt_len);
 
 #endif /* _NET_IOAM6_H */
diff --git a/include/uapi/linux/ioam6.h b/include/uapi/linux/ioam6.h
index 8f72b24fe..4ee764979 100644
--- a/include/uapi/linux/ioam6.h
+++ b/include/uapi/linux/ioam6.h
@@ -29,6 +29,7 @@ struct ioam6_hdr {
 	__u8 opt_len;
 	__u8 :8;				/* reserved */
 #define IOAM6_TYPE_PREALLOC 0
+#define IOAM6_TYPE_DEX 4
 	__u8 type;
 } __attribute__((packed));
 
@@ -130,4 +131,140 @@ struct ioam6_trace_hdr {
 	__u8	data[];
 } __attribute__((packed));
 
+/*
+ * IOAM Direct EXporting (DEX) Header
+ */
+struct ioam6_dex_hdr {
+	__be16	namespace_id;
+
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	/* Flags */
+	union {
+		__u8 flags;
+		struct {
+			__u8	:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1; /* unused */
+		};
+	};
+
+	/* Extension-Flags */
+	union {
+		__u8 extflags;
+		struct {
+			__u8	:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				seqNum:1, /* unused */
+				flowId:1; /* unused */
+		};
+	};
+
+	/* Trace Type and Reserved */
+	union {
+		__be32 type_be32;
+		struct {
+			__u32	bit7:1,		/* must be 0 per RFC 9326 */
+				bit6:1,
+				bit5:1,
+				bit4:1,
+				bit3:1,
+				bit2:1,
+				bit1:1,
+				bit0:1,
+				bit15:1,	/* unused */
+				bit14:1,	/* unused */
+				bit13:1,	/* unused */
+				bit12:1,	/* unused */
+				bit11:1,
+				bit10:1,
+				bit9:1,
+				bit8:1,
+				bit23:1,	/* reserved */
+				bit22:1,
+				bit21:1,	/* unused */
+				bit20:1,	/* unused */
+				bit19:1,	/* unused */
+				bit18:1,	/* unused */
+				bit17:1,	/* unused */
+				bit16:1,	/* unused */
+				:8;		/* reserved */
+		} type;
+	};
+#elif defined(__BIG_ENDIAN_BITFIELD)
+	/* Flags */
+	union {
+		__u8 flags;
+		struct {
+			__u8	:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1; /* unused */
+		};
+	};
+
+	/* Extension-Flags */
+	union {
+		__u8 extflags;
+		struct {
+			__u8	flowId:1, /* unused */
+				seqNum:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1, /* unused */
+				:1; /* unused */
+		};
+	};
+
+	/* Trace Type and Reserved */
+	union {
+		__be32 type_be32;
+		struct {
+			__u32	bit0:1,
+				bit1:1,
+				bit2:1,
+				bit3:1,
+				bit4:1,
+				bit5:1,
+				bit6:1,
+				bit7:1,		/* must be 0 per RFC 9326 */
+				bit8:1,
+				bit9:1,
+				bit10:1,
+				bit11:1,
+				bit12:1,	/* unused */
+				bit13:1,	/* unused */
+				bit14:1,	/* unused */
+				bit15:1,	/* unused */
+				bit16:1,	/* unused */
+				bit17:1,	/* unused */
+				bit18:1,	/* unused */
+				bit19:1,	/* unused */
+				bit20:1,	/* unused */
+				bit21:1,	/* unused */
+				bit22:1,
+				bit23:1,	/* reserved */
+				:8;		/* reserved */
+		} type;
+	};
+#else
+#error "Please fix <asm/byteorder.h>"
+#endif
+	__u8	extflags_data[];
+} __attribute__((packed));
+
 #endif /* _UAPI_LINUX_IOAM6_H */
diff --git a/include/uapi/linux/ioam6_genl.h b/include/uapi/linux/ioam6_genl.h
index 1733fbc51..56e8a65a9 100644
--- a/include/uapi/linux/ioam6_genl.h
+++ b/include/uapi/linux/ioam6_genl.h
@@ -54,6 +54,7 @@ enum {
 enum ioam6_event_type {
 	IOAM6_EVENT_UNSPEC,
 	IOAM6_EVENT_TRACE,
+	IOAM6_EVENT_DEX,
 };
 
 enum ioam6_event_attr {
@@ -64,6 +65,38 @@ enum ioam6_event_attr {
 	IOAM6_EVENT_ATTR_TRACE_TYPE,		/* u32 */
 	IOAM6_EVENT_ATTR_TRACE_DATA,		/* Binary */
 
+	IOAM6_EVENT_ATTR_OPTION_TYPE,		/* u8 */
+
+	IOAM6_EVENT_ATTR_DEX_NAMESPACE,		/* u16 */
+	IOAM6_EVENT_ATTR_DEX_FLOW_ID,		/* u32 */
+	IOAM6_EVENT_ATTR_DEX_SEQ_NUM,		/* u32 */
+
+	/* attr for ioam data */
+	IOAM6_EVENT_ATTR_DEX_DATA_HOP_LIM_NODE_ID,		/* u32 */
+	IOAM6_EVENT_ATTR_DEX_DATA_INGRESS_EGRESS_INTERFACES,	/* u32 */
+	IOAM6_EVENT_ATTR_DEX_DATA_TIMESTAMP,			/* u32 */
+	IOAM6_EVENT_ATTR_DEX_DATA_TIMESTAMP_FRAC,		/* u32 */
+	IOAM6_EVENT_ATTR_DEX_DATA_TRANSIT,			/* u32 */
+	IOAM6_EVENT_ATTR_DEX_DATA_NAMESPACE_SPECIFIC,		/* u32 */
+	IOAM6_EVENT_ATTR_DEX_DATA_QUEUE_DEPTH,			/* u32 */
+	IOAM6_EVENT_ATTR_DEX_DATA_CHECKSUM,			/* u32 */
+	IOAM6_EVENT_ATTR_DEX_DATA_HOP_LIM_NODE_ID_WIDE,		/* u64 */
+	IOAM6_EVENT_ATTR_DEX_DATA_INGRESS_EGRESS_INTERFACES_WIDE,/* u64 */
+	IOAM6_EVENT_ATTR_DEX_DATA_NAMESPACE_SPECIFIC_WIDE,	/* u64 */
+	IOAM6_EVENT_ATTR_DEX_DATA_BUFFER_OCCUPANCY,		/* u32 */
+	IOAM6_EVENT_ATTR_DEX_BIT_12,				/* undefined */
+	IOAM6_EVENT_ATTR_DEX_BIT_13,				/* undefined */
+	IOAM6_EVENT_ATTR_DEX_BIT_14,				/* undefined */
+	IOAM6_EVENT_ATTR_DEX_BIT_15,				/* undefined */
+	IOAM6_EVENT_ATTR_DEX_BIT_16,				/* undefined */
+	IOAM6_EVENT_ATTR_DEX_BIT_17,				/* undefined */
+	IOAM6_EVENT_ATTR_DEX_BIT_18,				/* undefined */
+	IOAM6_EVENT_ATTR_DEX_BIT_19,				/* undefined */
+	IOAM6_EVENT_ATTR_DEX_BIT_20,				/* undefined */
+	IOAM6_EVENT_ATTR_DEX_BIT_21,				/* undefined */
+	IOAM6_EVENT_ATTR_DEX_OSS_SCID,				/* u32 */
+	IOAM6_EVENT_ATTR_DEX_OSS_DATA,				/* Binary */
+
 	__IOAM6_EVENT_ATTR_MAX
 };
 
diff --git a/include/uapi/linux/ioam6_iptunnel.h b/include/uapi/linux/ioam6_iptunnel.h
index 8aef21e4a..b1c21e74c 100644
--- a/include/uapi/linux/ioam6_iptunnel.h
+++ b/include/uapi/linux/ioam6_iptunnel.h
@@ -9,6 +9,22 @@
 #ifndef _UAPI_LINUX_IOAM6_IPTUNNEL_H
 #define _UAPI_LINUX_IOAM6_IPTUNNEL_H
 
+/* Option types:
+ *	- Pre-allocated Trace Option (PTO)
+ *	- Direct EXport (DEX)
+ */
+enum {
+	__IOAM6_OPTION_TYPE_MIN,
+
+	IOAM6_OPTION_TYPE_PTO,
+	IOAM6_OPTION_TYPE_DEX,
+
+	__IOAM6_OPTION_TYPE_MAX,
+};
+
+#define IOAM6_OPTION_TYPE_MIN (__IOAM6_OPTION_TYPE_MIN + 1)
+#define IOAM6_OPTION_TYPE_MAX (__IOAM6_OPTION_TYPE_MAX - 1)
+
 /* Encap modes:
  *  - inline: direct insertion
  *  - encap: ip6ip6 encapsulation
@@ -56,6 +72,12 @@ enum {
 	 */
 	IOAM6_IPTUNNEL_SRC,		/* struct in6_addr */
 
+	/* Option Type */
+	IOAM6_OPTION_TYPE,		/* u8 */
+
+	/* IOAM Direct Export Header */
+	IOAM6_IPTUNNEL_DEX,		/* struct ioam6_dex_hdr */
+
 	__IOAM6_IPTUNNEL_MAX,
 };
 
diff --git a/net/ipv6/exthdrs.c b/net/ipv6/exthdrs.c
index 6789623b2..4f645e7f4 100644
--- a/net/ipv6/exthdrs.c
+++ b/net/ipv6/exthdrs.c
@@ -906,6 +906,7 @@ static bool ipv6_hop_ioam(struct sk_buff *skb, int optoff)
 {
 	struct ioam6_trace_hdr *trace;
 	struct ioam6_namespace *ns;
+	struct ioam6_dex_hdr *dex;
 	struct ioam6_hdr *hdr;
 
 	/* Bad alignment (must be 4n-aligned) */
@@ -950,9 +951,31 @@ static bool ipv6_hop_ioam(struct sk_buff *skb, int optoff)
 
 		ioam6_fill_trace_data(skb, ns, trace, true);
 
-		ioam6_event(IOAM6_EVENT_TRACE, dev_net(skb->dev),
+		ioam6_event(IOAM6_EVENT_TRACE, skb, dev_net(skb->dev), ns,
 			    GFP_ATOMIC, (void *)trace, hdr->opt_len - 2);
 		break;
+	case IOAM6_TYPE_DEX:
+		if (hdr->opt_len < 2 + sizeof(*dex))
+			goto drop;
+
+		dex = (struct ioam6_dex_hdr *)((u8 *)hdr + sizeof(*hdr));
+		if (!dex)
+			goto drop;
+
+		if (hdr->opt_len < 2 + sizeof(*dex) + hweight32(dex->extflags) * 4)
+			goto drop;
+
+		ns = ioam6_namespace(dev_net(skb->dev), dex->namespace_id);
+		if (!ns)
+			goto ignore;
+
+		if (!skb_valid_dst(skb))
+			ip6_route_input(skb);
+
+		ioam6_event(IOAM6_EVENT_DEX, skb, dev_net(skb->dev), ns,
+			    GFP_ATOMIC, (void*)dex, hdr->opt_len - 2);
+
+		break;
 	default:
 		break;
 	}
diff --git a/net/ipv6/ioam6.c b/net/ipv6/ioam6.c
index 08c929513..c45c6977b 100644
--- a/net/ipv6/ioam6.c
+++ b/net/ipv6/ioam6.c
@@ -14,6 +14,7 @@
 #include <linux/ioam6_genl.h>
 #include <linux/rhashtable.h>
 #include <linux/netdevice.h>
+#include <linux/bitops.h>
 
 #include <net/addrconf.h>
 #include <net/genetlink.h>
@@ -30,6 +31,11 @@ static void ioam6_sc_release(struct ioam6_schema *sc)
 	kfree_rcu(sc, rcu);
 }
 
+static void ioam6_dex_release(struct ioam6_dex_flow *flow)
+{
+	kfree_rcu(flow, rcu);
+}
+
 static void ioam6_free_ns(void *ptr, void *arg)
 {
 	struct ioam6_namespace *ns = (struct ioam6_namespace *)ptr;
@@ -46,6 +52,14 @@ static void ioam6_free_sc(void *ptr, void *arg)
 		ioam6_sc_release(sc);
 }
 
+static void ioam6_free_dex(void *ptr, void *arg)
+{
+	struct ioam6_dex_flow *flow = (struct ioam6_dex_flow*) ptr;
+
+	if (flow)
+		ioam6_dex_release(flow);
+}
+
 static int ioam6_ns_cmpfn(struct rhashtable_compare_arg *arg, const void *obj)
 {
 	const struct ioam6_namespace *ns = obj;
@@ -60,6 +74,13 @@ static int ioam6_sc_cmpfn(struct rhashtable_compare_arg *arg, const void *obj)
 	return (sc->id != *(u32 *)arg->key);
 }
 
+static int ioam6_dex_cmpfn(struct rhashtable_compare_arg *arg, const void *obj)
+{
+	const struct ioam6_dex_flow *flow = obj;
+
+	return (flow->flowID != *(__be32 *)arg->key);
+}
+
 static const struct rhashtable_params rht_ns_params = {
 	.key_len		= sizeof(__be16),
 	.key_offset		= offsetof(struct ioam6_namespace, id),
@@ -76,6 +97,14 @@ static const struct rhashtable_params rht_sc_params = {
 	.obj_cmpfn		= ioam6_sc_cmpfn,
 };
 
+static const struct rhashtable_params rht_dex_params = {
+	.key_len		= sizeof(__be32),
+	.key_offset		= offsetof(struct ioam6_dex_flow, flowID),
+	.head_offset		= offsetof(struct ioam6_dex_flow, head),
+	.automatic_shrinking	= true,
+	.obj_cmpfn		= ioam6_dex_cmpfn,
+};
+
 static struct genl_family ioam6_genl_family;
 
 static const struct nla_policy ioam6_genl_policy_addns[] = {
@@ -619,6 +648,129 @@ static const struct genl_multicast_group ioam6_mcgrps[] = {
 				       .flags = GENL_MCAST_CAP_NET_ADMIN },
 };
 
+static u32 ioam6_get_bit_0(struct sk_buff *skb)
+{
+	u32 raw32;
+	u8 byte;
+
+	byte = ipv6_hdr(skb)->hop_limit;
+	raw32 = dev_net(skb_dst(skb)->dev)->ipv6.sysctl.ioam6_id;
+
+	return (byte << 24) | raw32;
+}
+
+static u32 ioam6_get_bit_1(struct sk_buff *skb)
+{
+	u16 raw16;
+	u32 raw32;
+
+	if (!skb->dev)
+		raw16 = IOAM6_U16_UNAVAILABLE;
+	else
+		raw16 = (__force u16)READ_ONCE(__in6_dev_get(skb->dev)->cnf.ioam6_id);
+
+	raw32 = raw16 << 16;
+
+	if (skb_dst(skb)->dev->flags & IFF_LOOPBACK)
+		raw16 = IOAM6_U16_UNAVAILABLE;
+	else
+		raw16 = (__force u16)READ_ONCE(__in6_dev_get(skb_dst(skb)->dev)->cnf.ioam6_id);
+
+	return raw32 | raw16;
+}
+
+static u32 ioam6_get_bit_2(struct sk_buff *skb)
+{
+	struct timespec64 ts;
+	ktime_t tstamp;
+
+	if (!skb->dev) {
+		return IOAM6_U32_UNAVAILABLE;
+	} else {
+		tstamp = skb_tstamp_cond(skb, true);
+		ts = ktime_to_timespec64(tstamp);
+		return (u32)ts.tv_sec;
+	}
+}
+
+static u32 ioam6_get_bit_3(struct sk_buff *skb)
+{
+	struct timespec64 ts;
+	ktime_t tstamp;
+
+	if (!skb->dev) {
+		return IOAM6_U32_UNAVAILABLE;
+	} else {
+		tstamp = skb_tstamp_cond(skb, true);
+		ts = ktime_to_timespec64(tstamp);
+		return (u32)(ts.tv_nsec / NSEC_PER_USEC);
+	}
+}
+
+static u32 ioam6_get_bit_5(struct ioam6_namespace *ns)
+{
+	return be32_to_cpu(ns->data);
+}
+
+static u32 ioam6_get_bit_6(struct sk_buff *skb)
+{
+	struct netdev_queue *queue;
+	struct Qdisc *qdisc;
+	__u32 qlen, backlog;
+
+	if (skb_dst(skb)->dev->flags & IFF_LOOPBACK) {
+		return IOAM6_U32_UNAVAILABLE;
+	} else {
+		queue = skb_get_tx_queue(skb_dst(skb)->dev, skb);
+		qdisc = rcu_dereference(queue->qdisc);
+		qdisc_qstats_qlen_backlog(qdisc, &qlen, &backlog);
+		return backlog;
+	}
+}
+
+static u64 ioam6_get_bit_8(struct sk_buff *skb)
+{
+	u64 raw64;
+	u8 byte;
+
+	byte = ipv6_hdr(skb)->hop_limit;
+	raw64 = dev_net(skb_dst(skb)->dev)->ipv6.sysctl.ioam6_id_wide;
+
+	return ((u64)byte << 56) | raw64;
+}
+
+static u64 ioam6_get_bit_9(struct sk_buff *skb)
+{
+	u32 raw32;
+	u64 raw64;
+
+	if (!skb->dev)
+		raw32 = IOAM6_U32_UNAVAILABLE;
+	else
+		raw32 = READ_ONCE(__in6_dev_get(skb->dev)->cnf.ioam6_id_wide);
+
+	raw64 = (u64)raw32 << 32;
+
+	if (skb_dst(skb)->dev->flags & IFF_LOOPBACK)
+		raw32 = IOAM6_U32_UNAVAILABLE;
+	else
+		raw32 = READ_ONCE(__in6_dev_get(skb_dst(skb)->dev)->cnf.ioam6_id_wide);
+
+	return raw64 | raw32;
+}
+
+static u64 ioam6_get_bit_10(struct ioam6_namespace *ns)
+{
+	return be64_to_cpu(ns->data_wide);
+}
+
+static void ioam6_get_bit_22(struct ioam6_schema *sc, u8 *buf)
+{
+	*(__be32 *)buf = sc->hdr;
+	memcpy(buf+sizeof(__be32), sc->data, sc->len);
+	return;
+}
+
 static int ioam6_event_put_trace(struct sk_buff *skb,
 				 struct ioam6_trace_hdr *trace,
 				 unsigned int len)
@@ -636,21 +788,213 @@ static int ioam6_event_put_trace(struct sk_buff *skb,
 	return 0;
 }
 
-void ioam6_event(enum ioam6_event_type type, struct net *net, gfp_t gfp,
-		 void *opt, unsigned int opt_len)
+static int ioam6_event_put_dex(struct sk_buff *skb, struct ioam6_namespace *ns,
+			       struct sk_buff *nl_skb, struct ioam6_dex_hdr *hdr)
+{
+	struct ioam6_schema *sc = NULL;
+	u32 flow, seq, res;
+
+	if (nla_put_u8(nl_skb, IOAM6_EVENT_ATTR_OPTION_TYPE,
+		       IOAM6_TYPE_DEX))
+		return -1;
+
+	if (nla_put_u16(nl_skb, IOAM6_EVENT_ATTR_DEX_NAMESPACE,
+			be16_to_cpu(hdr->namespace_id)))
+		return -1;
+
+	if (hdr->flowId) {
+		flow = ((u32*)(hdr->extflags_data))[0];
+		if (nla_put_u32(nl_skb, IOAM6_EVENT_ATTR_DEX_FLOW_ID,
+			       be32_to_cpu(flow)))
+			return -1;
+	}
+
+	if (hdr->seqNum) {
+		seq = ((u32*)(hdr->extflags_data))[1];
+		if (nla_put_u32(nl_skb, IOAM6_EVENT_ATTR_DEX_SEQ_NUM,
+			       be32_to_cpu(seq)))
+			return -1;
+	}
+
+	/* fill ioam data */
+	if (hdr->type.bit0 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_DATA_HOP_LIM_NODE_ID,
+			    cpu_to_be32(ioam6_get_bit_0(skb))))
+		return -1;
+
+	if (hdr->type.bit1 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_DATA_INGRESS_EGRESS_INTERFACES,
+			    cpu_to_be32(ioam6_get_bit_1(skb))))
+		return -1;
+
+	if (hdr->type.bit2 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_DATA_TIMESTAMP,
+			    cpu_to_be32(ioam6_get_bit_2(skb))))
+		return -1;
+
+	if (hdr->type.bit3 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_DATA_TIMESTAMP_FRAC,
+			    cpu_to_be32(ioam6_get_bit_3(skb))))
+		return -1;
+	
+	if (hdr->type.bit4 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_DATA_TRANSIT,
+			    cpu_to_be32(IOAM6_U32_UNAVAILABLE)))
+		return -1;
+
+	if (hdr->type.bit5 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_DATA_NAMESPACE_SPECIFIC,
+			    cpu_to_be32(ioam6_get_bit_5(ns))))
+		return -1;
+
+	if (hdr->type.bit6 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_DATA_QUEUE_DEPTH,
+			    cpu_to_be32(ioam6_get_bit_6(skb))))
+		return -1;
+	
+	if (hdr->type.bit7 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_DATA_CHECKSUM,
+			    cpu_to_be32(IOAM6_U32_UNAVAILABLE)))
+		return -1;
+
+	if (hdr->type.bit8 &&
+		nla_put_u64_64bit(nl_skb,
+				  IOAM6_EVENT_ATTR_DEX_DATA_HOP_LIM_NODE_ID_WIDE,
+				  cpu_to_be64(ioam6_get_bit_8(skb)),
+				  IOAM6_ATTR_PAD))
+		return -1;
+
+	if (hdr->type.bit9 &&
+		nla_put_u64_64bit(nl_skb,
+				  IOAM6_EVENT_ATTR_DEX_DATA_INGRESS_EGRESS_INTERFACES_WIDE,
+				  cpu_to_be64(ioam6_get_bit_9(skb)),
+				  IOAM6_ATTR_PAD))
+		return -1;
+
+	if (hdr->type.bit10 &&
+		nla_put_u64_64bit(nl_skb,
+				  IOAM6_EVENT_ATTR_DEX_DATA_NAMESPACE_SPECIFIC_WIDE,
+				  cpu_to_be64(ioam6_get_bit_10(ns)),
+				  IOAM6_ATTR_PAD))
+		return -1;
+	
+	if (hdr->type.bit11 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_DATA_BUFFER_OCCUPANCY,
+			    cpu_to_be32(IOAM6_U32_UNAVAILABLE)))
+		return -1;
+
+	if (hdr->type.bit12 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_BIT_12,
+			    cpu_to_be32(IOAM6_U32_UNAVAILABLE)))
+		return -1;
+
+	if (hdr->type.bit13 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_BIT_13,
+			    cpu_to_be32(IOAM6_U32_UNAVAILABLE)))
+		return -1;
+
+	if (hdr->type.bit14 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_BIT_14,
+			    cpu_to_be32(IOAM6_U32_UNAVAILABLE)))
+		return -1;
+
+	if (hdr->type.bit15 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_BIT_15,
+			    cpu_to_be32(IOAM6_U32_UNAVAILABLE)))
+		return -1;
+
+	if (hdr->type.bit16 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_BIT_16,
+			    cpu_to_be32(IOAM6_U32_UNAVAILABLE)))
+		return -1;
+
+	if (hdr->type.bit17 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_BIT_17,
+			    cpu_to_be32(IOAM6_U32_UNAVAILABLE)))
+		return -1;
+
+	if (hdr->type.bit18 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_BIT_18,
+			    cpu_to_be32(IOAM6_U32_UNAVAILABLE)))
+		return -1;
+
+	if (hdr->type.bit19 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_BIT_19,
+			    cpu_to_be32(IOAM6_U32_UNAVAILABLE)))
+		return -1;
+
+	if (hdr->type.bit20 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_BIT_20,
+			    cpu_to_be32(IOAM6_U32_UNAVAILABLE)))
+		return -1;
+
+	if (hdr->type.bit21 &&
+		nla_put_u32(nl_skb,
+			    IOAM6_EVENT_ATTR_DEX_BIT_21,
+			    cpu_to_be32(IOAM6_U32_UNAVAILABLE)))
+		return -1;
+
+	if (hdr->type.bit22) {
+		rcu_read_lock();
+		sc = rcu_dereference(ns->schema);
+		if (!sc) {
+			rcu_read_unlock();
+			goto no_schema;
+		}
+
+		if (nla_put_u32(nl_skb, IOAM6_EVENT_ATTR_DEX_OSS_SCID,
+				cpu_to_be32(sc->id))) {
+			rcu_read_unlock();
+			return -1;
+		}
+
+		res = nla_put(nl_skb,
+			      IOAM6_EVENT_ATTR_DEX_OSS_DATA,
+			      sc->len, sc->data);
+
+		rcu_read_unlock();
+
+		if (res)
+			return -1;
+	}
+no_schema:
+	return 0;
+}
+
+void ioam6_event(enum ioam6_event_type type, struct sk_buff *skb,
+		 struct net *net, struct ioam6_namespace *ns,
+		 gfp_t gfp, void *opt, unsigned int opt_len)
 {
+	struct sk_buff *nl_skb;
 	struct nlmsghdr *nlh;
-	struct sk_buff *skb;
 
 	if (!genl_has_listeners(&ioam6_genl_family, net,
 				IOAM6_GENL_EV_GRP_OFFSET))
 		return;
 
-	skb = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);
-	if (!skb)
+	nl_skb = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);
+	if (!nl_skb)
 		return;
 
-	nlh = genlmsg_put(skb, 0, 0, &ioam6_genl_family, 0, type);
+	nlh = genlmsg_put(nl_skb, 0, 0, &ioam6_genl_family, 0, type);
 	if (!nlh)
 		goto nla_put_failure;
 
@@ -659,19 +1003,23 @@ void ioam6_event(enum ioam6_event_type type, struct net *net, gfp_t gfp,
 		WARN_ON_ONCE(1);
 		break;
 	case IOAM6_EVENT_TRACE:
-		if (ioam6_event_put_trace(skb, (struct ioam6_trace_hdr *)opt,
+		if (ioam6_event_put_trace(nl_skb, (struct ioam6_trace_hdr *)opt,
 					  opt_len))
 			goto nla_put_failure;
 		break;
+	case IOAM6_EVENT_DEX:
+		if (ioam6_event_put_dex(skb, ns, nl_skb, (struct ioam6_dex_hdr *)opt))
+			goto nla_put_failure;
+		break;
 	}
 
-	genlmsg_end(skb, nlh);
-	genlmsg_multicast_netns(&ioam6_genl_family, net, skb, 0,
+	genlmsg_end(nl_skb, nlh);
+	genlmsg_multicast_netns(&ioam6_genl_family, net, nl_skb, 0,
 				IOAM6_GENL_EV_GRP_OFFSET, gfp);
 	return;
 
 nla_put_failure:
-	nlmsg_free(skb);
+	nlmsg_free(nl_skb);
 }
 
 static struct genl_family ioam6_genl_family __ro_after_init = {
@@ -694,78 +1042,72 @@ struct ioam6_namespace *ioam6_namespace(struct net *net, __be16 id)
 	return rhashtable_lookup_fast(&nsdata->namespaces, &id, rht_ns_params);
 }
 
+struct ioam6_dex_flow *ioam6_dex_flow_insert(struct net *net, __be32 flowID)
+{
+	struct ioam6_pernet_data *nsdata;
+	struct ioam6_dex_flow *flow;
+	u32 seq, err = 0;
+
+	nsdata = ioam6_pernet(net);
+	if (!nsdata)
+		return NULL;
+
+	mutex_lock(&nsdata->lock);
+
+	flow = rhashtable_lookup_fast(&nsdata->dex_flows,
+				      &flowID, rht_dex_params);
+	if (flow) {
+		seq = be32_to_cpu(flow->seqID) + 1;
+		flow->seqID = cpu_to_be32(seq);
+		goto out_unlock;
+	}
+
+	flow = kzalloc(sizeof(*flow), GFP_ATOMIC);
+	if (!flow)
+		goto out_unlock;
+
+	flow->flowID = flowID;
+	flow->seqID = cpu_to_be32(0);
+
+	err = rhashtable_lookup_insert_fast(&nsdata->dex_flows,
+					    &flow->head, rht_dex_params);
+
+out_unlock:
+	mutex_unlock(&nsdata->lock);
+	return err == 0 ? flow : NULL;
+}
+
 static void __ioam6_fill_trace_data(struct sk_buff *skb,
 				    struct ioam6_namespace *ns,
 				    struct ioam6_trace_hdr *trace,
 				    struct ioam6_schema *sc,
 				    u8 sclen, bool is_input)
 {
-	struct timespec64 ts;
-	ktime_t tstamp;
-	u64 raw64;
-	u32 raw32;
-	u16 raw16;
 	u8 *data;
-	u8 byte;
 
 	data = trace->data + trace->remlen * 4 - trace->nodelen * 4 - sclen * 4;
 
 	/* hop_lim and node_id */
 	if (trace->type.bit0) {
-		byte = ipv6_hdr(skb)->hop_limit;
-		if (is_input)
-			byte--;
-
-		raw32 = dev_net(skb_dst(skb)->dev)->ipv6.sysctl.ioam6_id;
-
-		*(__be32 *)data = cpu_to_be32((byte << 24) | raw32);
+		*(__be32 *)data = cpu_to_be32(ioam6_get_bit_0(skb));
 		data += sizeof(__be32);
 	}
 
 	/* ingress_if_id and egress_if_id */
 	if (trace->type.bit1) {
-		if (!skb->dev)
-			raw16 = IOAM6_U16_UNAVAILABLE;
-		else
-			raw16 = (__force u16)READ_ONCE(__in6_dev_get(skb->dev)->cnf.ioam6_id);
-
-		*(__be16 *)data = cpu_to_be16(raw16);
-		data += sizeof(__be16);
-
-		if (skb_dst(skb)->dev->flags & IFF_LOOPBACK)
-			raw16 = IOAM6_U16_UNAVAILABLE;
-		else
-			raw16 = (__force u16)READ_ONCE(__in6_dev_get(skb_dst(skb)->dev)->cnf.ioam6_id);
-
-		*(__be16 *)data = cpu_to_be16(raw16);
-		data += sizeof(__be16);
+		*(__be32 *)data = cpu_to_be32(ioam6_get_bit_1(skb));
+		data += sizeof(__be32);
 	}
 
 	/* timestamp seconds */
 	if (trace->type.bit2) {
-		if (!skb->dev) {
-			*(__be32 *)data = cpu_to_be32(IOAM6_U32_UNAVAILABLE);
-		} else {
-			tstamp = skb_tstamp_cond(skb, true);
-			ts = ktime_to_timespec64(tstamp);
-
-			*(__be32 *)data = cpu_to_be32((u32)ts.tv_sec);
-		}
+		*(__be32 *)data = cpu_to_be32(ioam6_get_bit_2(skb));
 		data += sizeof(__be32);
 	}
 
 	/* timestamp subseconds */
 	if (trace->type.bit3) {
-		if (!skb->dev) {
-			*(__be32 *)data = cpu_to_be32(IOAM6_U32_UNAVAILABLE);
-		} else {
-			if (!trace->type.bit2) {
-				tstamp = skb_tstamp_cond(skb, true);
-				ts = ktime_to_timespec64(tstamp);
-			}
-
-			*(__be32 *)data = cpu_to_be32((u32)(ts.tv_nsec / NSEC_PER_USEC));
-		}
+		*(__be32 *)data = cpu_to_be32(ioam6_get_bit_3(skb));
 		data += sizeof(__be32);
 	}
 
@@ -777,25 +1119,13 @@ static void __ioam6_fill_trace_data(struct sk_buff *skb,
 
 	/* namespace data */
 	if (trace->type.bit5) {
-		*(__be32 *)data = ns->data;
+		*(__be32 *)data = cpu_to_be32(ioam6_get_bit_5(ns));
 		data += sizeof(__be32);
 	}
 
 	/* queue depth */
 	if (trace->type.bit6) {
-		struct netdev_queue *queue;
-		struct Qdisc *qdisc;
-		__u32 qlen, backlog;
-
-		if (skb_dst(skb)->dev->flags & IFF_LOOPBACK) {
-			*(__be32 *)data = cpu_to_be32(IOAM6_U32_UNAVAILABLE);
-		} else {
-			queue = skb_get_tx_queue(skb_dst(skb)->dev, skb);
-			qdisc = rcu_dereference(queue->qdisc);
-			qdisc_qstats_qlen_backlog(qdisc, &qlen, &backlog);
-
-			*(__be32 *)data = cpu_to_be32(backlog);
-		}
+		*(__be32 *)data = cpu_to_be32(ioam6_get_bit_6(skb));
 		data += sizeof(__be32);
 	}
 
@@ -807,38 +1137,19 @@ static void __ioam6_fill_trace_data(struct sk_buff *skb,
 
 	/* hop_lim and node_id (wide) */
 	if (trace->type.bit8) {
-		byte = ipv6_hdr(skb)->hop_limit;
-		if (is_input)
-			byte--;
-
-		raw64 = dev_net(skb_dst(skb)->dev)->ipv6.sysctl.ioam6_id_wide;
-
-		*(__be64 *)data = cpu_to_be64(((u64)byte << 56) | raw64);
+		*(__be64 *)data = cpu_to_be64(ioam6_get_bit_8(skb));
 		data += sizeof(__be64);
 	}
 
 	/* ingress_if_id and egress_if_id (wide) */
 	if (trace->type.bit9) {
-		if (!skb->dev)
-			raw32 = IOAM6_U32_UNAVAILABLE;
-		else
-			raw32 = READ_ONCE(__in6_dev_get(skb->dev)->cnf.ioam6_id_wide);
-
-		*(__be32 *)data = cpu_to_be32(raw32);
-		data += sizeof(__be32);
-
-		if (skb_dst(skb)->dev->flags & IFF_LOOPBACK)
-			raw32 = IOAM6_U32_UNAVAILABLE;
-		else
-			raw32 = READ_ONCE(__in6_dev_get(skb_dst(skb)->dev)->cnf.ioam6_id_wide);
-
-		*(__be32 *)data = cpu_to_be32(raw32);
-		data += sizeof(__be32);
+		*(__be64 *)data = cpu_to_be64(ioam6_get_bit_9(skb));
+		data += sizeof(__be64);
 	}
 
 	/* namespace data (wide) */
 	if (trace->type.bit10) {
-		*(__be64 *)data = ns->data_wide;
+		*(__be64 *)data = cpu_to_be64(ioam6_get_bit_10(ns));
 		data += sizeof(__be64);
 	}
 
@@ -913,10 +1224,7 @@ static void __ioam6_fill_trace_data(struct sk_buff *skb,
 		if (!sc) {
 			*(__be32 *)data = cpu_to_be32(IOAM6_U32_UNAVAILABLE >> 8);
 		} else {
-			*(__be32 *)data = sc->hdr;
-			data += sizeof(__be32);
-
-			memcpy(data, sc->data, sc->len);
+			ioam6_get_bit_22(sc, data);
 		}
 	}
 }
@@ -979,8 +1287,14 @@ static int __net_init ioam6_net_init(struct net *net)
 	if (err)
 		goto free_rht_ns;
 
+	err = rhashtable_init(&nsdata->dex_flows, &rht_dex_params);
+	if (err)
+		goto free_rht_sc;
+
 out:
 	return err;
+free_rht_sc:
+	rhashtable_destroy(&nsdata->schemas);
 free_rht_ns:
 	rhashtable_destroy(&nsdata->namespaces);
 free_nsdata:
@@ -995,6 +1309,7 @@ static void __net_exit ioam6_net_exit(struct net *net)
 
 	rhashtable_free_and_destroy(&nsdata->namespaces, ioam6_free_ns, NULL);
 	rhashtable_free_and_destroy(&nsdata->schemas, ioam6_free_sc, NULL);
+	rhashtable_free_and_destroy(&nsdata->dex_flows, ioam6_free_dex, NULL);
 
 	kfree(nsdata);
 }
diff --git a/net/ipv6/ioam6_iptunnel.c b/net/ipv6/ioam6_iptunnel.c
index beb6b4cfc..1a5f5add1 100644
--- a/net/ipv6/ioam6_iptunnel.c
+++ b/net/ipv6/ioam6_iptunnel.c
@@ -12,6 +12,7 @@
 #include <linux/in6.h>
 #include <linux/ioam6.h>
 #include <linux/ioam6_iptunnel.h>
+#include <linux/bitops.h>
 #include <net/dst.h>
 #include <net/sock.h>
 #include <net/lwtunnel.h>
@@ -32,6 +33,13 @@ struct ioam6_lwt_encap {
 	struct ioam6_trace_hdr traceh;
 } __packed;
 
+struct ioam6_lwt_dex {
+	struct ipv6_hopopt_hdr eh;
+	u8 pad[2];			/* 2-octet padding for 4n-alignment */
+	struct ioam6_hdr ioamh;
+	struct ioam6_dex_hdr dexh;
+} __packed;
+
 struct ioam6_lwt_freq {
 	u32 k;
 	u32 n;
@@ -41,11 +49,16 @@ struct ioam6_lwt {
 	struct dst_cache cache;
 	struct ioam6_lwt_freq freq;
 	atomic_t pkt_cnt;
+	u8 optionType;
 	u8 mode;
 	bool has_tunsrc;
 	struct in6_addr tunsrc;
 	struct in6_addr tundst;
-	struct ioam6_lwt_encap tuninfo;
+	/* will never have both at the same time */
+	union {
+		struct ioam6_lwt_encap tuninfo;
+		struct ioam6_lwt_dex dexinfo;
+	};
 };
 
 static const struct netlink_range_validation freq_range = {
@@ -63,12 +76,25 @@ static struct ioam6_lwt_encap *ioam6_lwt_info(struct lwtunnel_state *lwt)
 	return &ioam6_lwt_state(lwt)->tuninfo;
 }
 
+static struct ioam6_lwt_dex *ioam6_lwt_dex_info(struct lwtunnel_state *lwt)
+{
+	return &ioam6_lwt_state(lwt)->dexinfo;
+}
+
 static struct ioam6_trace_hdr *ioam6_lwt_trace(struct lwtunnel_state *lwt)
 {
 	return &(ioam6_lwt_state(lwt)->tuninfo.traceh);
 }
 
+static struct ioam6_dex_hdr *ioam6_lwt_dex_hdr(struct lwtunnel_state *lwt)
+{
+	return &(ioam6_lwt_state(lwt)->dexinfo.dexh);
+}
+
 static const struct nla_policy ioam6_iptunnel_policy[IOAM6_IPTUNNEL_MAX + 1] = {
+	[IOAM6_OPTION_TYPE] = NLA_POLICY_RANGE(NLA_U8,
+		IOAM6_OPTION_TYPE_MIN,
+		IOAM6_OPTION_TYPE_MAX),
 	[IOAM6_IPTUNNEL_FREQ_K] = NLA_POLICY_FULL_RANGE(NLA_U32, &freq_range),
 	[IOAM6_IPTUNNEL_FREQ_N] = NLA_POLICY_FULL_RANGE(NLA_U32, &freq_range),
 	[IOAM6_IPTUNNEL_MODE]	= NLA_POLICY_RANGE(NLA_U8,
@@ -76,6 +102,8 @@ static const struct nla_policy ioam6_iptunnel_policy[IOAM6_IPTUNNEL_MAX + 1] = {
 						   IOAM6_IPTUNNEL_MODE_MAX),
 	[IOAM6_IPTUNNEL_SRC]	= NLA_POLICY_EXACT_LEN(sizeof(struct in6_addr)),
 	[IOAM6_IPTUNNEL_DST]	= NLA_POLICY_EXACT_LEN(sizeof(struct in6_addr)),
+	[IOAM6_IPTUNNEL_DEX]	= NLA_POLICY_EXACT_LEN(
+					sizeof(struct ioam6_dex_hdr)),
 	[IOAM6_IPTUNNEL_TRACE]	= NLA_POLICY_EXACT_LEN(
 					sizeof(struct ioam6_trace_hdr)),
 };
@@ -103,6 +131,19 @@ static bool ioam6_validate_trace_hdr(struct ioam6_trace_hdr *trace)
 	return true;
 }
 
+static bool ioam6_validate_dex_hdr(struct ioam6_dex_hdr *dexh)
+{
+	/* per rfc 9326, checksum complement (bit 7) must be 0 */
+	if (!dexh->type_be32 || dexh->type.bit7 |
+	    dexh->type.bit12 | dexh->type.bit13 | dexh->type.bit14 |
+	    dexh->type.bit15 | dexh->type.bit16 | dexh->type.bit17 |
+	    dexh->type.bit18 | dexh->type.bit19 | dexh->type.bit20 |
+	    dexh->type.bit21)
+		return false;
+
+	return true;
+}
+
 static int ioam6_build_state(struct net *net, struct nlattr *nla,
 			     unsigned int family, const void *cfg,
 			     struct lwtunnel_state **ts,
@@ -110,12 +151,15 @@ static int ioam6_build_state(struct net *net, struct nlattr *nla,
 {
 	struct nlattr *tb[IOAM6_IPTUNNEL_MAX + 1];
 	struct ioam6_lwt_encap *tuninfo;
+	struct ioam6_lwt_dex *dexinfo;
 	struct ioam6_trace_hdr *trace;
 	struct lwtunnel_state *lwt;
+	struct ioam6_dex_hdr *dex;
 	struct ioam6_lwt *ilwt;
 	int len_aligned, err;
+	u8 optionType, mode;
 	u32 freq_k, freq_n;
-	u8 mode;
+	u8 extflags_size;
 
 	if (family != AF_INET6)
 		return -EINVAL;
@@ -157,32 +201,94 @@ static int ioam6_build_state(struct net *net, struct nlattr *nla,
 		return -EINVAL;
 	}
 
-	if (!tb[IOAM6_IPTUNNEL_TRACE]) {
-		NL_SET_ERR_MSG(extack, "missing trace");
-		return -EINVAL;
-	}
+	if (!tb[IOAM6_OPTION_TYPE])
+		optionType = IOAM6_OPTION_TYPE_PTO;
+	else
+		optionType = nla_get_u8(tb[IOAM6_OPTION_TYPE]);
 
-	trace = nla_data(tb[IOAM6_IPTUNNEL_TRACE]);
-	if (!ioam6_validate_trace_hdr(trace)) {
-		NL_SET_ERR_MSG_ATTR(extack, tb[IOAM6_IPTUNNEL_TRACE],
-				    "invalid trace validation");
-		return -EINVAL;
-	}
+	if (optionType == IOAM6_OPTION_TYPE_PTO) {
+		if (!tb[IOAM6_IPTUNNEL_TRACE]) {
+			NL_SET_ERR_MSG(extack, "missing trace");
+			return -EINVAL;
+		}
+
+		trace = nla_data(tb[IOAM6_IPTUNNEL_TRACE]);
+		if (!ioam6_validate_trace_hdr(trace)) {
+			NL_SET_ERR_MSG_ATTR(extack, tb[IOAM6_IPTUNNEL_TRACE],
+					    "invalid trace validation");
+			return -EINVAL;
+		}
 
-	len_aligned = ALIGN(trace->remlen * 4, 8);
-	lwt = lwtunnel_state_alloc(sizeof(*ilwt) + len_aligned);
-	if (!lwt)
-		return -ENOMEM;
+		len_aligned = ALIGN(trace->remlen * 4, 8);
+		lwt = lwtunnel_state_alloc(sizeof(*ilwt) + len_aligned);
+		if (!lwt)
+			return -ENOMEM;
+
+		ilwt = ioam6_lwt_state(lwt);
+		err = dst_cache_init(&ilwt->cache, GFP_ATOMIC);
+		if (err)
+			goto free_lwt;
+
+		tuninfo = ioam6_lwt_info(lwt);
+		tuninfo->eh.hdrlen = ((sizeof(*tuninfo) + len_aligned) >> 3) - 1;
+		tuninfo->pad[0] = IPV6_TLV_PADN;
+		tuninfo->ioamh.type = IOAM6_TYPE_PREALLOC;
+		tuninfo->ioamh.opt_type = IPV6_TLV_IOAM;
+		tuninfo->ioamh.opt_len = sizeof(tuninfo->ioamh) - 2 + sizeof(*trace)
+						+ trace->remlen * 4;
+
+		memcpy(&tuninfo->traceh, trace, sizeof(*trace));
+
+		if (len_aligned - trace->remlen * 4) {
+			tuninfo->traceh.data[trace->remlen * 4] = IPV6_TLV_PADN;
+			tuninfo->traceh.data[trace->remlen * 4 + 1] = 2;
+		}
+	} else if (optionType == IOAM6_OPTION_TYPE_DEX) {
+		if (!tb[IOAM6_IPTUNNEL_DEX]) {
+			NL_SET_ERR_MSG(extack, "missing dex");
+			return -EINVAL;
+		}
+
+		dex = nla_data(tb[IOAM6_IPTUNNEL_DEX]);
+		if (!ioam6_validate_dex_hdr(dex)) {
+			NL_SET_ERR_MSG_ATTR(extack, tb[IOAM6_IPTUNNEL_DEX],
+					    "invalid dex validation");
+			return -EINVAL;
+		}
+
+		extflags_size = hweight8(dex->extflags) * 4;
+		len_aligned = ALIGN(extflags_size, 8);
+		lwt = lwtunnel_state_alloc(sizeof(*ilwt) + len_aligned);
+		if (!lwt)
+			return -ENOMEM;
+
+		ilwt = ioam6_lwt_state(lwt);
+		err = dst_cache_init(&ilwt->cache, GFP_ATOMIC);
+		if (err)
+			goto free_lwt;
+
+		dexinfo = ioam6_lwt_dex_info(lwt);
+		dexinfo->eh.hdrlen = ((sizeof(*dexinfo) + len_aligned) >> 3) - 1;
+		dexinfo->pad[0] = IPV6_TLV_PADN;
+		dexinfo->ioamh.opt_type = IPV6_TLV_IOAM;
+		dexinfo->ioamh.type = IOAM6_TYPE_DEX;
+		dexinfo->ioamh.opt_len = sizeof(dexinfo->ioamh) - 2 + sizeof(*dex)
+						+ extflags_size;
+
+		memcpy(&dexinfo->dexh, dex, sizeof(*dex));
+		if (len_aligned - extflags_size) {
+			dexinfo->dexh.extflags_data[extflags_size] = IPV6_TLV_PADN;
+			dexinfo->dexh.extflags_data[extflags_size + 1] = 2;
+		}
+	} else {
+		NL_SET_ERR_MSG(extack, "unknown option type");
+	}
 
-	ilwt = ioam6_lwt_state(lwt);
-	err = dst_cache_init(&ilwt->cache, GFP_ATOMIC);
-	if (err)
-		goto free_lwt;
 
 	atomic_set(&ilwt->pkt_cnt, 0);
 	ilwt->freq.k = freq_k;
 	ilwt->freq.n = freq_n;
-
+	ilwt->optionType = optionType;
 	ilwt->mode = mode;
 
 	if (!tb[IOAM6_IPTUNNEL_SRC]) {
@@ -210,21 +316,6 @@ static int ioam6_build_state(struct net *net, struct nlattr *nla,
 		}
 	}
 
-	tuninfo = ioam6_lwt_info(lwt);
-	tuninfo->eh.hdrlen = ((sizeof(*tuninfo) + len_aligned) >> 3) - 1;
-	tuninfo->pad[0] = IPV6_TLV_PADN;
-	tuninfo->ioamh.type = IOAM6_TYPE_PREALLOC;
-	tuninfo->ioamh.opt_type = IPV6_TLV_IOAM;
-	tuninfo->ioamh.opt_len = sizeof(tuninfo->ioamh) - 2 + sizeof(*trace)
-					+ trace->remlen * 4;
-
-	memcpy(&tuninfo->traceh, trace, sizeof(*trace));
-
-	if (len_aligned - trace->remlen * 4) {
-		tuninfo->traceh.data[trace->remlen * 4] = IPV6_TLV_PADN;
-		tuninfo->traceh.data[trace->remlen * 4 + 1] = 2;
-	}
-
 	lwt->type = LWTUNNEL_ENCAP_IOAM6;
 	lwt->flags |= LWTUNNEL_STATE_OUTPUT_REDIRECT;
 
@@ -254,13 +345,60 @@ static int ioam6_do_fill(struct net *net, struct sk_buff *skb)
 	return 0;
 }
 
+static __be32 ioam6_dex_flowid(struct sk_buff *skb)
+{
+	u32 hash = skb_get_hash(skb);
+	hash = rol32(hash, 16);
+
+	return (__force __be32)hash;
+}
+
+static void ioam6_dex_extflags(struct sk_buff *skb, struct net *net, struct ioam6_lwt *ilwt)
+{
+	struct ioam6_pernet_data *pernet;
+	struct ioam6_dex_flow *flow;
+	struct ipv6hdr *hdr;
+	__be32 flowID;
+	u8 *data;
+
+	hdr = ipv6_hdr(skb);
+
+	flowID = ioam6_dex_flowid(skb);
+
+	pernet = ioam6_pernet(net);
+	if (!pernet)
+		return;
+
+	flow = ioam6_dex_flow_insert(net, flowID);
+	if (!flow)
+		return;
+
+	data = ilwt->dexinfo.dexh.extflags_data;
+
+	if (ilwt->dexinfo.dexh.flowId) {
+		*(__be32 *)data = flowID;
+		data += sizeof(__be32);
+	}
+
+	if (ilwt->dexinfo.dexh.seqNum)
+		*(__be32 *)data = cpu_to_be32(be32_to_cpu(flow->seqID) + 1);
+
+	return;
+}
+
 static int ioam6_do_inline(struct net *net, struct sk_buff *skb,
-			   struct ioam6_lwt_encap *tuninfo)
+			   struct ioam6_lwt *ilwt)
 {
 	struct ipv6hdr *oldhdr, *hdr;
+	struct ioam6_namespace *ns;
 	int hdrlen, err;
 
-	hdrlen = (tuninfo->eh.hdrlen + 1) << 3;
+	if (ilwt->optionType == IOAM6_OPTION_TYPE_PTO)
+		hdrlen = (ilwt->tuninfo.eh.hdrlen + 1) << 3;
+	else if (ilwt->optionType == IOAM6_OPTION_TYPE_DEX)
+		hdrlen = (ilwt->dexinfo.eh.hdrlen + 1) << 3;
+	else
+		return -1;
 
 	err = skb_cow_head(skb, hdrlen + skb->mac_len);
 	if (unlikely(err))
@@ -276,30 +414,62 @@ static int ioam6_do_inline(struct net *net, struct sk_buff *skb,
 
 	hdr = ipv6_hdr(skb);
 	memmove(hdr, oldhdr, sizeof(*oldhdr));
-	tuninfo->eh.nexthdr = hdr->nexthdr;
+	if (ilwt->optionType == IOAM6_OPTION_TYPE_PTO)
+		ilwt->tuninfo.eh.nexthdr = hdr->nexthdr;
+	else if (ilwt->optionType == IOAM6_OPTION_TYPE_DEX)
+		ilwt->dexinfo.eh.nexthdr = hdr->nexthdr;
+	else
+		return -1;
 
 	skb_set_transport_header(skb, sizeof(*hdr));
 	skb_postpush_rcsum(skb, hdr, sizeof(*hdr) + hdrlen);
 
-	memcpy(skb_transport_header(skb), (u8 *)tuninfo, hdrlen);
+	if (ilwt->optionType == IOAM6_OPTION_TYPE_PTO) {
+		memcpy(skb_transport_header(skb), (u8 *)&(ilwt->tuninfo), hdrlen);
+	} else if (ilwt->optionType == IOAM6_OPTION_TYPE_DEX) {
+		if (ilwt->dexinfo.dexh.extflags)
+			ioam6_dex_extflags(skb, net, ilwt);
+		memcpy(skb_transport_header(skb), (u8 *)&(ilwt->dexinfo), hdrlen);
+	} else {
+		return -1;
+	}
 
 	hdr->nexthdr = NEXTHDR_HOP;
 	hdr->payload_len = cpu_to_be16(skb->len - sizeof(*hdr));
 
-	return ioam6_do_fill(net, skb);
+	if (ilwt->optionType == IOAM6_OPTION_TYPE_PTO) {
+		return ioam6_do_fill(net, skb);
+	} else if (ilwt->optionType == IOAM6_OPTION_TYPE_DEX) {
+		ns = ioam6_namespace(net, ilwt->dexinfo.dexh.namespace_id);
+		if (!ns)
+			return 0;
+
+		ioam6_event(IOAM6_EVENT_DEX, skb, net, ns, GFP_ATOMIC,
+			    (void*)&(ilwt->dexinfo.dexh),
+			    sizeof(struct ioam6_dex_hdr) +
+				4 * hweight32(ilwt->dexinfo.dexh.extflags));
+
+		return 0;
+	} else {
+		return -1;
+	}
 }
 
 static int ioam6_do_encap(struct net *net, struct sk_buff *skb,
-			  struct ioam6_lwt_encap *tuninfo,
-			  bool has_tunsrc,
-			  struct in6_addr *tunsrc,
-			  struct in6_addr *tundst)
+			  struct ioam6_lwt *ilwt)
 {
 	struct dst_entry *dst = skb_dst(skb);
 	struct ipv6hdr *hdr, *inner_hdr;
+	struct ioam6_namespace *ns;
 	int hdrlen, len, err;
 
-	hdrlen = (tuninfo->eh.hdrlen + 1) << 3;
+	if (ilwt->optionType == IOAM6_OPTION_TYPE_PTO)
+		hdrlen = (ilwt->tuninfo.eh.hdrlen + 1) << 3;
+	else if (ilwt->optionType == IOAM6_OPTION_TYPE_DEX)
+		hdrlen = (ilwt->dexinfo.eh.hdrlen + 1) << 3;
+	else
+		return -1;
+	
 	len = sizeof(*hdr) + hdrlen;
 
 	err = skb_cow_head(skb, len + skb->mac_len);
@@ -313,25 +483,54 @@ static int ioam6_do_encap(struct net *net, struct sk_buff *skb,
 	skb_mac_header_rebuild(skb);
 	skb_set_transport_header(skb, sizeof(*hdr));
 
-	tuninfo->eh.nexthdr = NEXTHDR_IPV6;
-	memcpy(skb_transport_header(skb), (u8 *)tuninfo, hdrlen);
+	if (ilwt->optionType == IOAM6_OPTION_TYPE_PTO)
+		ilwt->tuninfo.eh.nexthdr = NEXTHDR_IPV6;
+	else if (ilwt->optionType == IOAM6_OPTION_TYPE_DEX)
+		ilwt->dexinfo.eh.nexthdr = NEXTHDR_IPV6;
+	else
+		return -1;
+
+	if (ilwt->optionType == IOAM6_OPTION_TYPE_PTO) {
+		memcpy(skb_transport_header(skb), (u8 *)&(ilwt->tuninfo), hdrlen);
+	} else if (ilwt->optionType == IOAM6_OPTION_TYPE_DEX) {
+		if (ilwt->dexinfo.dexh.extflags)
+			ioam6_dex_extflags(skb, net, ilwt);
+		memcpy(skb_transport_header(skb), (u8 *)&(ilwt->dexinfo), hdrlen);
+	} else {
+		return -1;
+	}
 
 	hdr = ipv6_hdr(skb);
 	memcpy(hdr, inner_hdr, sizeof(*hdr));
 
 	hdr->nexthdr = NEXTHDR_HOP;
 	hdr->payload_len = cpu_to_be16(skb->len - sizeof(*hdr));
-	hdr->daddr = *tundst;
+	hdr->daddr = ilwt->tundst;
 
-	if (has_tunsrc)
-		memcpy(&hdr->saddr, tunsrc, sizeof(*tunsrc));
+	skb_postpush_rcsum(skb, hdr, len);
+	
+	if (ilwt->has_tunsrc)
+		memcpy(&hdr->saddr, &ilwt->tunsrc, sizeof(ilwt->tunsrc));
 	else
 		ipv6_dev_get_saddr(net, dst->dev, &hdr->daddr,
 				   IPV6_PREFER_SRC_PUBLIC, &hdr->saddr);
 
-	skb_postpush_rcsum(skb, hdr, len);
+	if (ilwt->optionType == IOAM6_OPTION_TYPE_PTO) {
+		return ioam6_do_fill(net, skb);
+	} else if (ilwt->optionType == IOAM6_OPTION_TYPE_DEX) {
+		ns = ioam6_namespace(net, ilwt->dexinfo.dexh.namespace_id);
+		if (!ns)
+			return 0;
+
+		ioam6_event(IOAM6_EVENT_DEX, skb, net, ns, GFP_ATOMIC,
+			    (void*)&(ilwt->dexinfo.dexh),
+			    sizeof(struct ioam6_dex_hdr) +
+				4 * hweight32(ilwt->dexinfo.dexh.extflags));
 
-	return ioam6_do_fill(net, skb);
+		return 0;
+	} else {
+		return -1;
+	}
 }
 
 static int ioam6_output(struct net *net, struct sock *sk, struct sk_buff *skb)
@@ -361,7 +560,7 @@ static int ioam6_output(struct net *net, struct sock *sk, struct sk_buff *skb)
 		if (ipv6_hdr(skb)->nexthdr == NEXTHDR_HOP)
 			goto out;
 
-		err = ioam6_do_inline(net, skb, &ilwt->tuninfo);
+		err = ioam6_do_inline(net, skb, ilwt);
 		if (unlikely(err))
 			goto drop;
 
@@ -369,9 +568,7 @@ static int ioam6_output(struct net *net, struct sock *sk, struct sk_buff *skb)
 	case IOAM6_IPTUNNEL_MODE_ENCAP:
 do_encap:
 		/* Encapsulation (ip6ip6) */
-		err = ioam6_do_encap(net, skb, &ilwt->tuninfo,
-				     ilwt->has_tunsrc, &ilwt->tunsrc,
-				     &ilwt->tundst);
+		err = ioam6_do_encap(net, skb, ilwt);
 		if (unlikely(err))
 			goto drop;
 
@@ -444,6 +641,10 @@ static int ioam6_fill_encap_info(struct sk_buff *skb,
 	struct ioam6_lwt *ilwt = ioam6_lwt_state(lwtstate);
 	int err;
 
+	err = nla_put_u8(skb, IOAM6_OPTION_TYPE, ilwt->optionType);
+	if (err)
+		goto ret;
+
 	err = nla_put_u32(skb, IOAM6_IPTUNNEL_FREQ_K, ilwt->freq.k);
 	if (err)
 		goto ret;
@@ -469,8 +670,14 @@ static int ioam6_fill_encap_info(struct sk_buff *skb,
 			goto ret;
 	}
 
-	err = nla_put(skb, IOAM6_IPTUNNEL_TRACE, sizeof(ilwt->tuninfo.traceh),
-		      &ilwt->tuninfo.traceh);
+	if (ilwt->optionType == IOAM6_OPTION_TYPE_PTO)
+		err = nla_put(skb, IOAM6_IPTUNNEL_TRACE, sizeof(ilwt->tuninfo.traceh),
+				&ilwt->tuninfo.traceh);
+	else if (ilwt->optionType == IOAM6_OPTION_TYPE_DEX)
+		err = nla_put(skb, IOAM6_IPTUNNEL_DEX, sizeof(ilwt->dexinfo.dexh),
+				&ilwt->dexinfo.dexh);
+	else
+		return -1;
 ret:
 	return err;
 }
@@ -480,10 +687,16 @@ static int ioam6_encap_nlsize(struct lwtunnel_state *lwtstate)
 	struct ioam6_lwt *ilwt = ioam6_lwt_state(lwtstate);
 	int nlsize;
 
-	nlsize = nla_total_size(sizeof(ilwt->freq.k)) +
+	nlsize = nla_total_size(sizeof(ilwt->optionType)) +
+		  nla_total_size(sizeof(ilwt->freq.k)) +
 		  nla_total_size(sizeof(ilwt->freq.n)) +
-		  nla_total_size(sizeof(ilwt->mode)) +
-		  nla_total_size(sizeof(ilwt->tuninfo.traceh));
+		  nla_total_size(sizeof(ilwt->mode));
+
+	if (ilwt->optionType == IOAM6_OPTION_TYPE_PTO)
+		  nlsize += nla_total_size(sizeof(ilwt->tuninfo.traceh));
+
+	if (ilwt->optionType == IOAM6_OPTION_TYPE_DEX)
+		nlsize += nla_total_size(sizeof(ilwt->dexinfo.dexh));
 
 	if (ilwt->mode != IOAM6_IPTUNNEL_MODE_INLINE) {
 		if (ilwt->has_tunsrc)
@@ -499,10 +712,13 @@ static int ioam6_encap_cmp(struct lwtunnel_state *a, struct lwtunnel_state *b)
 {
 	struct ioam6_trace_hdr *trace_a = ioam6_lwt_trace(a);
 	struct ioam6_trace_hdr *trace_b = ioam6_lwt_trace(b);
+	struct ioam6_dex_hdr *dex_a = ioam6_lwt_dex_hdr(a);
+	struct ioam6_dex_hdr *dex_b = ioam6_lwt_dex_hdr(b);
 	struct ioam6_lwt *ilwt_a = ioam6_lwt_state(a);
 	struct ioam6_lwt *ilwt_b = ioam6_lwt_state(b);
 
-	return (ilwt_a->freq.k != ilwt_b->freq.k ||
+	return (ilwt_a->optionType != ilwt_b->optionType ||
+		ilwt_a->freq.k != ilwt_b->freq.k ||
 		ilwt_a->freq.n != ilwt_b->freq.n ||
 		ilwt_a->mode != ilwt_b->mode ||
 		ilwt_a->has_tunsrc != ilwt_b->has_tunsrc ||
@@ -511,7 +727,10 @@ static int ioam6_encap_cmp(struct lwtunnel_state *a, struct lwtunnel_state *b)
 		(ilwt_a->mode != IOAM6_IPTUNNEL_MODE_INLINE &&
 		 ilwt_a->has_tunsrc &&
 		 !ipv6_addr_equal(&ilwt_a->tunsrc, &ilwt_b->tunsrc)) ||
-		trace_a->namespace_id != trace_b->namespace_id);
+		(ilwt_a->optionType == IOAM6_OPTION_TYPE_PTO &&
+		 trace_a->namespace_id != trace_b->namespace_id) ||
+		(ilwt_b->optionType == IOAM6_OPTION_TYPE_DEX &&
+		 dex_a->namespace_id != dex_b->namespace_id));
 }
 
 static const struct lwtunnel_encap_ops ioam6_iptun_ops = {
